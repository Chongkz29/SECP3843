{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/Ex2DKNFRp4MiBPzgmMzY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drshahizan/SECP3843/blob/main/submission/Terence172/question5/files/code/Question5_STDE_AA_TerenceLoorthanathan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive First... Upload the tweets dataset in here as well"
      ],
      "metadata": {
        "id": "6rrmTPl2Xxt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMYH8r0Icm-4",
        "outputId": "441e61e0-b86c-4879-a247-a56b4171242f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Change file_path accordingly and run the code\n",
        "- Note: I imported new_data.json earlier into my drive"
      ],
      "metadata": {
        "id": "c_l8rGScXxGH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l9KIOHrFcXCX"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Personal/new_data.json'\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    data = [json.loads(line) for line in file]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the Data\n",
        "\n",
        "Step 1) Import all required libraries"
      ],
      "metadata": {
        "id": "qHZRPRyE6Fi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "bqVx6rVL6SyG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2) Data preprocessing"
      ],
      "metadata": {
        "id": "wRwyMYtD6WeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_data(tweet):\n",
        "    # Remove Unwanted Fields\n",
        "    text = tweet['text']\n",
        "    retweet_count = tweet['retweet_count']\n",
        "\n",
        "\n",
        "    # Remove non-English characters, URLs, hashtags, and emojis from the tweet.\n",
        "    non_english_characters = re.compile(r'[^\\x00-\\x7F]')\n",
        "    url_pattern = re.compile(r'http\\S+|www\\S+')\n",
        "    hashtag_pattern = re.compile(r'#\\S+')\n",
        "    emoji_pattern = re.compile(r'[^\\w\\s]|_')\n",
        "\n",
        "    text = non_english_characters.sub('', text)\n",
        "    text = url_pattern.sub('', text)\n",
        "    text = hashtag_pattern.sub('', text)\n",
        "    text = emoji_pattern.sub('', text)\n",
        "\n",
        "    # If nothing is in text return nothing\n",
        "    if text.isspace():\n",
        "        return None\n",
        "\n",
        "    return text, retweet_count\n",
        "\n",
        "\n",
        "preprocessed_data = [preprocess_data(tweet) for tweet in data[0]]\n",
        "\n",
        "# Print original and preprocessed text for a few tweets\n",
        "for i in range(10):\n",
        "    print(\"Preprocessed Tokens:\", preprocessed_data[i])\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDIKUlhd6Ycy",
        "outputId": "98187441-5572-49af-f3a5-3fa9744da93d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed Tokens: ('eu preciso de terminar de fazer a minha tabela est muito foda ', None)\n",
            "\n",
            "Preprocessed Tokens: ('I cant wait for ', None)\n",
            "\n",
            "Preprocessed Tokens: ('Oky nenek nya RT wikigehol Oky jd anak na yyyy RT okyoktaaaaa Papanya asil yaaa  RT cacaamarisa Eh wikigehol tidur sana Udah male', None)\n",
            "\n",
            "Preprocessed Tokens: None\n",
            "\n",
            "Preprocessed Tokens: ('AdmireBiebs what ya think about to change my name to NickJMunroC I want one with them both theyre my imaginary husbands P', None)\n",
            "\n",
            "Preprocessed Tokens: ('First week of school is over P', None)\n",
            "\n",
            "Preprocessed Tokens: ('fair today then jersey shoreD', None)\n",
            "\n",
            "Preprocessed Tokens: ('teetoolegit lmfao No BS hahaha', None)\n",
            "\n",
            "Preprocessed Tokens: ('RT ayatquran Sesungguhnya shalat itu adalah kewajiban yang ditentukan waktunya atas orangorang yang beriman 4103', None)\n",
            "\n",
            "Preprocessed Tokens: ('RT Philanthropy How should nonprofit groups measure their socialmedia efforts A new podcast from afine ', None)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The End : Produced by Terence"
      ],
      "metadata": {
        "id": "wa-PHIhOMey9"
      }
    }
  ]
}